{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40086f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd4a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4597fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "positive_tweets[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4798c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7887f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1dd37ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@groovinshawn', 'NN'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('rechargeable', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('it', 'PRP'),\n",
       " ('normally', 'RB'),\n",
       " ('comes', 'VBZ'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('charger', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('u', 'JJ'),\n",
       " ('buy', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " (':)', 'JJ')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af0f2523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f8c0a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets: [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\n",
      "lemma names: ['car', 'auto', 'automobile', 'machine', 'motorcar']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "word_synset = wn.synsets(\"car\")\n",
    "print(\"synsets:\", word_synset)\n",
    "print(\"lemma names:\", word_synset[0].lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d207087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1206f7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28f89404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wheeled vehicle adapted to the rails of railroad'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[1].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf2453c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three cars had jumped the rails']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[1].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "032aeb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2732bda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31119d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'living_thing.n.01', 'organism.n.01', 'plant.n.02', 'vascular_plant.n.01', 'woody_plant.n.01', 'tree.n.01']\n"
     ]
    }
   ],
   "source": [
    "tree = wn.synsets(\"tree\")[0]\n",
    "paths = tree.hypernym_paths()\n",
    "for p in paths:\n",
    "    print([synset.name() for synset in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25a8ae79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('burl.n.02'),\n",
       " Synset('crown.n.07'),\n",
       " Synset('limb.n.02'),\n",
       " Synset('stump.n.01'),\n",
       " Synset('trunk.n.01')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ff479b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('heartwood.n.01'), Synset('sapwood.n.01')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3353357f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('forest.n.01')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "440f94da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "tokens = tweet_tokens[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8660cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24032f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@groovinshawn',\n",
       " 'they',\n",
       " 'be',\n",
       " 'rechargeable',\n",
       " 'and',\n",
       " 'it',\n",
       " 'normally',\n",
       " 'come',\n",
       " 'with',\n",
       " 'a',\n",
       " 'charger',\n",
       " 'when',\n",
       " 'u',\n",
       " 'buy',\n",
       " 'it',\n",
       " ':)']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return 'n'  \n",
    "    elif tag.startswith('VB'):\n",
    "        return 'v'  \n",
    "    elif tag.startswith('JJ'):\n",
    "        return 'a'  \n",
    "    else:\n",
    "        return 'a'  \n",
    "\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatized_sentence = []\n",
    "    \n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    for token, tag in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(tag)\n",
    "\n",
    "        lemmatized_token = lemmatizer.lemmatize(token, wordnet_pos)\n",
    "        lemmatized_sentence.append(lemmatized_token)\n",
    "    \n",
    "    return lemmatized_sentence\n",
    "\n",
    "lemmatize_sentence(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7772b7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65c60ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "a\n",
      "about\n",
      "above\n",
      "after\n",
      "again\n",
      "against\n",
      "ain\n",
      "all\n",
      "am\n",
      "an\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "for i in range(10):\n",
    "    print(stop_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a3dd4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "After: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def process_tokens(tweet_tokens):\n",
    "    cleaned_tokens = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        \n",
    "        if re.match(r'http[s]?://\\S+', token):\n",
    "            continue\n",
    "        \n",
    "        if token.startswith('@'):\n",
    "            continue\n",
    "\n",
    "        if token.lower() in stop_words:\n",
    "            continue\n",
    "            \n",
    "        if token in string.punctuation:\n",
    "            continue\n",
    "            \n",
    "        if token.startswith('#'):\n",
    "            continue\n",
    "        \n",
    "        token = token.lower()\n",
    "        wordnet_pos = get_wordnet_pos(tag)\n",
    "        synsets = wn.synsets(token, pos=wordnet_pos)\n",
    "        \n",
    "        if synsets:\n",
    "            lemmatized_token = synsets[0].lemmas()[0].name()\n",
    "        else:\n",
    "            lemmatized_token = token\n",
    "        \n",
    "        cleaned_tokens.append(lemmatized_token)\n",
    "    \n",
    "    return cleaned_tokens\n",
    "\n",
    "print(\"Before:\", tweet_tokens[50])\n",
    "print(\"After:\", process_tokens(tweet_tokens[50]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d3a122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', ':d']\n"
     ]
    }
   ],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = [process_tokens(tweet) for tweet in positive_tweet_tokens]\n",
    "negative_cleaned_tokens_list = [process_tokens(tweet) for tweet in negative_tweet_tokens]\n",
    "\n",
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "648a7aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words from all positive tokens: ['top', 'prosecute', 'member', 'community', 'week', ':)', 'hey', 'James', 'odd', ':/']\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    all_words = []\n",
    "    for tweet in cleaned_tokens_list:\n",
    "        all_words.extend(tweet)\n",
    "    return all_words\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "print(\"First 10 words from all positive tokens:\", all_pos_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8356cbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 383), ('follow', 362), ('love', 337), ('...', 290), ('get', 269), ('thank', 258), ('good', 238)]\n"
     ]
    }
   ],
   "source": [
    "fdist_pos = FreqDist(all_pos_words)\n",
    "\n",
    "print(fdist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64bfa388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic distance between 'sister' and 'brother': 12\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def semantic_distance(word1, word2):\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "\n",
    "    if not synsets1 or not synsets2:\n",
    "        return float('inf')  \n",
    "\n",
    "    min_distance = float('inf')\n",
    "\n",
    "    for syn1 in synsets1:\n",
    "        for syn2 in synsets2:\n",
    "            common_hypernyms = syn1.common_hypernyms(syn2) \n",
    "            \n",
    "            if common_hypernyms:\n",
    "                lch = common_hypernyms[0] \n",
    "                distance = syn1.shortest_path_distance(lch) + syn2.shortest_path_distance(lch)\n",
    "                min_distance = min(min_distance, distance)\n",
    "\n",
    "    return min_distance if min_distance != float('inf') else None  \n",
    "\n",
    "\n",
    "word1 = \"sister\"\n",
    "word2 = \"brother\"\n",
    "print(f\"Semantic distance between '{word1}' and '{word2}':\", semantic_distance(word1, word2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69a99f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentiment', 'text'],\n",
       "        num_rows: 179995\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentiment', 'text'],\n",
       "        num_rows: 44999\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gxb912/large-twitter-tweets-sentiment\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca4acd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['top', 'prosecute', 'member', 'community', 'week', ':)']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0fd8fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_dict(tokens):\n",
    "    return dict([token, True] for token in tokens)\n",
    "    \n",
    "def get_tweets_for_model(cleaned_tokens_list):   \n",
    "    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aef29efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3ea2d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9956666666666667\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2065.4 : 1.0\n",
      "                      :) = True           Positi : Negati =   1659.6 : 1.0\n",
      "                     sad = True           Negati : Positi =     26.0 : 1.0\n",
      "                 welcome = True           Positi : Negati =     23.1 : 1.0\n",
      "                     Bam = True           Positi : Negati =     20.4 : 1.0\n",
      "                  arrive = True           Positi : Negati =     19.9 : 1.0\n",
      "                followed = True           Negati : Positi =     13.7 : 1.0\n",
      "                    miss = True           Negati : Positi =     13.6 : 1.0\n",
      "                 web_log = True           Positi : Negati =     12.4 : 1.0\n",
      "                   didnt = True           Negati : Positi =     12.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a995ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3710830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_tweet = \"the service was not good\"\n",
    "\n",
    "custom_tokens = process_tokens(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(get_token_dict(custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ce554b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad :  Negative\n",
      "service is bad :  Negative\n",
      "service is really bad :  Negative\n",
      "service is so terrible :  Negative\n",
      "great service :  Positive\n",
      "they gave me their money :  Negative\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(text):\n",
    "    custom_tokens = process_tokens(word_tokenize(text))\n",
    "    return classifier.classify(get_token_dict(custom_tokens))\n",
    "\n",
    "texts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they gave me their money\"]\n",
    "for t in texts:\n",
    "    print(t, \": \", get_sentiment(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ec099bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Accuracy: 0.7401942265383675\n",
      "Logistic Regression Accuracy: 0.7892397608835752\n",
      "Naive Bayes Accuracy: 0.7401942265383675\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk import classify\n",
    "\n",
    "def process_tokens(tokens):\n",
    "    return [token.lower() for token in tokens if token.isalpha()]\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    processed_data = []\n",
    "    for tweet, label in zip(dataset['text'], dataset['sentiment']):\n",
    "        tokens = process_tokens(word_tokenize(tweet))\n",
    "        processed_data.append((' '.join(tokens), 'Positive' if label == 1 else 'Negative'))\n",
    "    return processed_data\n",
    "\n",
    "train_data = preprocess_data(dataset['train'])\n",
    "test_data = preprocess_data(dataset['test'])\n",
    "\n",
    "train_data_nb = [(dict([(token, True) for token in process_tokens(word_tokenize(tweet))]), label) for tweet, label in train_data]\n",
    "test_data_nb = [(dict([(token, True) for token in process_tokens(word_tokenize(tweet))]), label) for tweet, label in test_data]\n",
    "\n",
    "naive_bayes_classifier = NaiveBayesClassifier.train(train_data_nb)\n",
    "\n",
    "naive_bayes_accuracy = classify.accuracy(naive_bayes_classifier, test_data_nb)\n",
    "print(\"Naive Bayes Classifier Accuracy:\", naive_bayes_accuracy)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "train_texts = [tweet for tweet, _ in train_data]\n",
    "train_labels = [label for _, label in train_data]\n",
    "\n",
    "test_texts = [tweet for tweet, _ in test_data]\n",
    "test_labels = [label for _, label in test_data]\n",
    "\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "logreg_classifier = LogisticRegression(max_iter=1000)\n",
    "logreg_classifier.fit(X_train, train_labels)\n",
    "logreg_predictions = logreg_classifier.predict(X_test)\n",
    "\n",
    "logreg_accuracy = accuracy_score(test_labels, logreg_predictions)\n",
    "print(\"Logistic Regression Accuracy:\", logreg_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bfc152d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.64      0.88      0.74     18967\n",
      "    Positive       0.88      0.64      0.74     26032\n",
      "\n",
      "    accuracy                           0.74     44999\n",
      "   macro avg       0.76      0.76      0.74     44999\n",
      "weighted avg       0.78      0.74      0.74     44999\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.77      0.71      0.74     18967\n",
      "    Positive       0.80      0.85      0.82     26032\n",
      "\n",
      "    accuracy                           0.79     44999\n",
      "   macro avg       0.79      0.78      0.78     44999\n",
      "weighted avg       0.79      0.79      0.79     44999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "naive_bayes_predictions = [naive_bayes_classifier.classify(tweet) for tweet, _ in test_data_nb]\n",
    "print(\"Naive Bayes Classification Report:\")\n",
    "print(classification_report(test_labels, naive_bayes_predictions))\n",
    "\n",
    "\n",
    "logreg_predictions = logreg_classifier.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(test_labels, logreg_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd0ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
